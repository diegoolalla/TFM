# GuÃ­a de Inicio RÃ¡pido - TFM

Esta guÃ­a te ayudarÃ¡ a comenzar con tu TFM en pocos minutos.

## ğŸ“‹ Pre-requisitos

- Python 3.8 o superior
- pip (gestor de paquetes de Python)
- Git
- Jupyter Notebook o JupyterLab

## ğŸš€ ConfiguraciÃ³n Inicial

### 1. Clonar el Repositorio

```bash
git clone https://github.com/diegoolalla/TFM.git
cd TFM
```

### 2. Crear Entorno Virtual

**OpciÃ³n A: Con venv (recomendado)**
```bash
# Crear entorno virtual
python -m venv venv

# Activar en Linux/Mac
source venv/bin/activate

# Activar en Windows
venv\Scripts\activate
```

**OpciÃ³n B: Con conda**
```bash
conda create -n tfm python=3.8
conda activate tfm
```

### 3. Instalar Dependencias

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

### 4. Verificar InstalaciÃ³n

```bash
python -c "import pandas, numpy, sklearn, matplotlib; print('âœ… Todas las librerÃ­as instaladas correctamente!')"
```

---

## ğŸ“Š Trabajar con tu Dataset

### Paso 1: Obtener un Dataset

Elige un dataset pÃºblico de:
- [Kaggle Datasets](https://www.kaggle.com/datasets)
- [UCI ML Repository](https://archive.ics.uci.edu/ml/index.php)
- [Google Dataset Search](https://datasetsearch.research.google.com/)

### Paso 2: Colocar el Dataset

```bash
# Crear subdirectorio para datos crudos
mkdir -p data/raw

# Copiar tu dataset
cp /ruta/a/tu/dataset.csv data/raw/
```

### Paso 3: Documentar el Dataset

Crea un archivo `data/DATASET.md` con informaciÃ³n:

```markdown
# Nombre del Dataset

- **Fuente**: [URL]
- **Fecha de descarga**: [Fecha]
- **Licencia**: [Tipo de licencia]
- **DescripciÃ³n**: [Breve descripciÃ³n]

## CaracterÃ­sticas
- NÃºmero de instancias: [N]
- NÃºmero de caracterÃ­sticas: [M]
- Variable objetivo: [nombre]

## Columnas
- columna1: [descripciÃ³n]
- columna2: [descripciÃ³n]
...
```

---

## ğŸ’» Trabajar con el Notebook

### OpciÃ³n 1: Jupyter Notebook

```bash
jupyter notebook notebooks/analisis_tfm.ipynb
```

### OpciÃ³n 2: JupyterLab (recomendado)

```bash
jupyter lab notebooks/analisis_tfm.ipynb
```

### OpciÃ³n 3: VS Code

1. Instala la extensiÃ³n de Python
2. Instala la extensiÃ³n de Jupyter
3. Abre el archivo `.ipynb`

### OpciÃ³n 4: Google Colab

1. Sube el notebook a Google Drive
2. Abre con Google Colab
3. Sube tu dataset o conÃ©ctalo desde Drive

---

## ğŸ“ Flujo de Trabajo Recomendado

### Fase 1: ExploraciÃ³n (Semana 1-2)

1. **FamiliarÃ­zate con el dataset**
   - Ejecuta las primeras secciones del notebook
   - Explora las distribuciones
   - Identifica problemas de calidad de datos

2. **AnÃ¡lisis descriptivo**
   - Genera estadÃ­sticas descriptivas
   - Crea visualizaciones
   - Identifica patrones y correlaciones

### Fase 2: Preprocesamiento (Semana 2-3)

1. **Limpieza de datos**
   - Maneja valores nulos
   - Identifica y trata outliers
   - Elimina duplicados

2. **Transformaciones**
   - Codifica variables categÃ³ricas
   - Normaliza/estandariza variables numÃ©ricas
   - Crea nuevas caracterÃ­sticas si es necesario

### Fase 3: ModelizaciÃ³n (Semana 3-4)

1. **Implementa mÃºltiples modelos**
   - Comienza con modelos simples (baseline)
   - Avanza a modelos mÃ¡s complejos
   - Prueba al menos 5-7 modelos diferentes

2. **ValidaciÃ³n**
   - Usa validaciÃ³n cruzada
   - Compara mÃ©tricas
   - Selecciona el mejor modelo

### Fase 4: OptimizaciÃ³n (Semana 4-5)

1. **Ajusta hiperparÃ¡metros**
   - Grid Search o Random Search
   - Optimiza el mejor modelo
   - Valida resultados

2. **AnÃ¡lisis de errores**
   - Estudia predicciones incorrectas
   - Identifica patrones de error
   - PropÃ³n mejoras

### Fase 5: DocumentaciÃ³n (Semana 5-6)

1. **Informe escrito**
   - Usa la plantilla en `docs/plantilla_informe.md`
   - MÃ¡ximo 20 pÃ¡ginas
   - Incluye visualizaciones clave

2. **Video explicativo**
   - Usa la guÃ­a en `docs/guion_video.md`
   - 5 minutos exactos
   - Formato MP4

3. **PublicaciÃ³n (opcional)**
   - Sigue la guÃ­a en `docs/guia_kaggle.md`
   - Comparte en Kaggle
   - Promociona en LinkedIn/Twitter

---

## ğŸ› ï¸ Uso de Utilidades

### Cargar y Explorar Datos

```python
from src.data_utils import load_dataset, get_basic_info

# Cargar dataset
df = load_dataset('data/raw/mi_dataset.csv')

# InformaciÃ³n bÃ¡sica
info = get_basic_info(df)
print(f"Shape: {info['shape']}")
print(f"Missing values: {info['missing_values']}")
```

### Visualizaciones

```python
from src.visualization import plot_distribution, plot_correlation_matrix

# Distribuciones
plot_distribution(df, save_path='results/visualizations/dist.png')

# Correlaciones
plot_correlation_matrix(df, save_path='results/visualizations/corr.png')
```

### EvaluaciÃ³n de Modelos

```python
from src.model_evaluation import evaluate_classification_model, compare_models

# Evaluar modelo
metrics = evaluate_classification_model(y_test, y_pred, model_name='Random Forest')
print(f"Accuracy: {metrics['accuracy']:.4f}")

# Comparar mÃºltiples modelos
results = {...}  # Diccionario con resultados
compare_models(results, metric='accuracy', save_path='results/visualizations/comparison.png')
```

---

## ğŸ“ Estructura de Archivos Durante el Desarrollo

```
TFM/
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ analisis_tfm.ipynb          # Tu notebook principal (modificar)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ mi_dataset.csv          # Tu dataset (aÃ±adir)
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â””â”€â”€ clean_data.csv          # Datos procesados (generar)
â”‚   â””â”€â”€ DATASET.md                  # DocumentaciÃ³n (crear)
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ random_forest.pkl           # Modelo guardado (generar)
â”‚   â””â”€â”€ model_config.json           # ConfiguraciÃ³n (generar)
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ visualizations/
â”‚   â”‚   â”œâ”€â”€ eda_*.png               # GrÃ¡ficos EDA (generar)
â”‚   â”‚   â”œâ”€â”€ model_*.png             # GrÃ¡ficos modelos (generar)
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ metrics/
â”‚       â”œâ”€â”€ model_comparison.csv    # ComparaciÃ³n (generar)
â”‚       â””â”€â”€ best_model_metrics.json # MÃ©tricas finales (generar)
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ informe_tfm.pdf             # Informe final (crear)
â”‚   â””â”€â”€ presentacion.pptx           # Diapositivas (crear)
â””â”€â”€ docs/
    â””â”€â”€ notas.md                    # Tus notas (opcional)
```

---

## âš ï¸ Problemas Comunes

### Problema: "Module not found"

**SoluciÃ³n:**
```bash
# AsegÃºrate de estar en el entorno virtual
# Reinstala las dependencias
pip install -r requirements.txt
```

### Problema: "FileNotFoundError" al cargar datos

**SoluciÃ³n:**
```python
# Verifica la ruta relativa correcta
# Desde el notebook:
df = pd.read_csv('../data/raw/mi_dataset.csv')

# O usa ruta absoluta
import os
data_path = os.path.join(os.getcwd(), '..', 'data', 'raw', 'mi_dataset.csv')
df = pd.read_csv(data_path)
```

### Problema: Jupyter no encuentra las utilidades

**SoluciÃ³n:**
```python
# AÃ±ade el directorio padre al path
import sys
sys.path.append('..')

# Ahora importa
from src.data_utils import load_dataset
```

### Problema: Memoria insuficiente

**SoluciÃ³n:**
```python
# Trabaja con una muestra del dataset
df_sample = df.sample(frac=0.1, random_state=42)

# O carga solo columnas necesarias
df = pd.read_csv('data.csv', usecols=['col1', 'col2', 'target'])

# O usa chunks
for chunk in pd.read_csv('data.csv', chunksize=10000):
    # Procesa cada chunk
    pass
```

---

## ğŸ“š Recursos Adicionales

### Tutoriales
- [Scikit-learn Documentation](https://scikit-learn.org/stable/tutorial/index.html)
- [Pandas Tutorials](https://pandas.pydata.org/docs/getting_started/tutorials.html)
- [Kaggle Learn](https://www.kaggle.com/learn)

### Ejemplos de TFM
- Busca "master thesis machine learning" en GitHub
- Explora notebooks en Kaggle con tag "thesis"

### Comunidades
- [r/MachineLearning](https://www.reddit.com/r/MachineLearning/)
- [Stack Overflow](https://stackoverflow.com/questions/tagged/machine-learning)
- [Kaggle Discussion](https://www.kaggle.com/discussion)

---

## âœ… Checklist de Progreso

### Semana 1-2: ExploraciÃ³n
- [ ] Dataset seleccionado y descargado
- [ ] Entorno configurado
- [ ] AnÃ¡lisis descriptivo completado
- [ ] Primeras visualizaciones creadas

### Semana 2-3: Preprocesamiento
- [ ] Valores nulos manejados
- [ ] Outliers analizados
- [ ] Variables transformadas
- [ ] Datos divididos en train/test

### Semana 3-4: ModelizaciÃ³n
- [ ] Baseline model implementado
- [ ] 5+ modelos entrenados
- [ ] ValidaciÃ³n cruzada aplicada
- [ ] Mejor modelo identificado

### Semana 4-5: OptimizaciÃ³n
- [ ] HiperparÃ¡metros optimizados
- [ ] AnÃ¡lisis de errores realizado
- [ ] CaracterÃ­sticas importantes identificadas
- [ ] Resultados finales validados

### Semana 5-6: DocumentaciÃ³n
- [ ] Notebook limpio y documentado
- [ ] Informe escrito (â‰¤20 pÃ¡ginas)
- [ ] Video grabado (5 minutos)
- [ ] CÃ³digo en GitHub
- [ ] (Opcional) Publicado en Kaggle

---

## ğŸ¯ Consejos Finales

1. **Empieza simple**: No intentes hacer todo perfecto desde el inicio
2. **Itera**: El anÃ¡lisis de datos es iterativo, no lineal
3. **Documenta mientras trabajas**: No dejes la documentaciÃ³n para el final
4. **Pide feedback**: Muestra tu trabajo a compaÃ±eros o mentores
5. **Gestiona el tiempo**: Usa la planificaciÃ³n semanal sugerida
6. **Haz commits frecuentes**: Usa Git para versionar tu trabajo
7. **Backup**: MantÃ©n copias de seguridad de tu trabajo

---

**Â¡Mucho Ã©xito con tu TFM!** ğŸ“ğŸš€

Para preguntas o problemas, consulta la documentaciÃ³n en la carpeta `docs/` o abre un issue en GitHub.
